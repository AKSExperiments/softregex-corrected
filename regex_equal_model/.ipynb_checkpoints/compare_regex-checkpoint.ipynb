{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LSTM,Embedding,Linear\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class compare_regex(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, target_size):\n",
    "        super(compare_regex, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embed = Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm1 = LSTM(embedding_dim ,hidden_dim, bidirectional=True, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1, batch_first=True)\n",
    "        self.fc1 = Linear(hidden_dim*2*2, 60)\n",
    "        self.fc2 = Linear(60, 20)\n",
    "        self.fc3 = Linear(20, target_size)\n",
    "\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        if torch.cuda.is_available():\n",
    "            return (torch.zeros(2, bs, self.hidden_dim).cuda(),\n",
    "                   torch.zeros(2, bs, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(2, bs, self.hidden_dim),\n",
    "                   torch.zeros(2, bs, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, bs, line1, line2, input1_lengths,input2_lengths):\n",
    "        embeded1 = self.embed(line1)\n",
    "        embeded2 = self.embed(line2)\n",
    "        hidden1 = self.init_hidden(bs)\n",
    "        lstm1_out, last_hidden1 = self.lstm1(embeded1,hidden1)\n",
    "        hidden2 = self.init_hidden(bs)\n",
    "        lstm2_out, last_hidden2 = self.lstm1(embeded2,hidden2)\n",
    "        fc1_out = self.fc1(torch.cat((lstm1_out.mean(1), lstm2_out.mean(1)),1))\n",
    "\n",
    "        fc1_out = F.tanh(fc1_out)\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        fc2_out = F.tanh(fc2_out)\n",
    "        fc3_out = self.fc3(fc2_out)\n",
    "        score = F.log_softmax(fc3_out,dim=1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../pair_data/train_data.txt','r')\n",
    "\n",
    "total_set = set()\n",
    "lines1 = list()\n",
    "lines2 = list()\n",
    "targets = list()\n",
    "\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    splitted = line.split('\\t')\n",
    "    a = '{}\\t{}\\t{}'.format(splitted[1],splitted[0],splitted[2])\n",
    "    if not a in total_set:\n",
    "        total_set.add(line)\n",
    "        \n",
    "train_lines1 = list()\n",
    "train_lines2 = list()\n",
    "train_targets = list()\n",
    "for line in total_set:\n",
    "    splitted = line.split('\\t')\n",
    "    train_lines1.append(splitted[0])\n",
    "    train_lines2.append(splitted[1])\n",
    "    train_targets.append(splitted[2])\n",
    "    train_lines1.append(splitted[1])\n",
    "    train_lines2.append(splitted[0])\n",
    "    train_targets.append(splitted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../pair_data/test_data.txt','r')\n",
    "\n",
    "total_set = set()\n",
    "lines1 = list()\n",
    "lines2 = list()\n",
    "targets = list()\n",
    "\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    splitted = line.split('\\t')\n",
    "    a = '{}\\t{}\\t{}'.format(splitted[1],splitted[0],splitted[2])\n",
    "    if not a in total_set:\n",
    "        total_set.add(line)\n",
    "        \n",
    "test_lines1 = list()\n",
    "test_lines2 = list()\n",
    "test_targets = list()\n",
    "for line in total_set:\n",
    "    splitted = line.split('\\t')\n",
    "    test_lines1.append(splitted[0])\n",
    "    test_lines2.append(splitted[1])\n",
    "    test_targets.append(splitted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete cell\n",
    "f = open('../pair_data/data_pairs_test(4_depth).txt','r')\n",
    "\n",
    "total_set = set()\n",
    "lines1 = list()\n",
    "lines2 = list()\n",
    "targets = list()\n",
    "\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    splitted = line.split('\\t')\n",
    "    a = '{}\\t{}\\t{}'.format(splitted[1],splitted[0],splitted[2])\n",
    "    if not a in total_set:\n",
    "        total_set.add(line)\n",
    "        \n",
    "test_lines1 = list()\n",
    "test_lines2 = list()\n",
    "test_targets = list()\n",
    "for line in total_set:\n",
    "    splitted = line.split('\\t')\n",
    "    test_lines1.append(splitted[0])\n",
    "    test_lines2.append(splitted[1])\n",
    "    test_targets.append(splitted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372728\n",
      "5297\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lines1))\n",
    "print(len(test_lines1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vocab = {w:i for i,w in enumerate(set([t for s in train_lines1 for t in s.split(' ')]), 1)}\n",
    "vocab['<pad>'] = 0\n",
    "vocab_size = len(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 17, '(': 1, '[': 18, '3': 19, '5': 2, '<NUM>': 3, '<M0>': 21, '}': 22, '2': 23, '<CAP>': 4, ',': 24, '<M1>': 25, '4': 5, '{': 6, ')': 7, '<LET>': 8, '\\\\': 9, '.': 27, '<M3>': 10, '&': 11, '<pad>': 0, '*': 12, ']': 13, '6': 15, '<M2>': 30, '|': 28, '<VOW>': 20, '7': 26, '+': 29, '~': 14, '<LOW>': 16}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_w = open('./compare_vocab(uncleaned).txt','w')\n",
    "# for i in vocab.items():\n",
    "#     f_w.write('{}\\t{}\\n'.format(i[0],i[1]))\n",
    "# f_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "f = open('./compare_vocab.txt','r')\n",
    "for i in f.read().splitlines():\n",
    "    splitted = i.split('\\t')\n",
    "    vocab[splitted[0]] = int(splitted[1])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def make_input_seq(lines1, lines2, targets):\n",
    "    max_len = 40\n",
    "    lines1_seq2idx = list()\n",
    "    lines2_seq2idx = list()\n",
    "    targets_idx = list()\n",
    "    lines1_seq = [s.split() for s in lines1]\n",
    "    lines2_seq = [s.split() for s in lines2]\n",
    "    for line_num in range(len(lines1_seq)):\n",
    "        if len(lines1_seq[line_num]) > max_len or len(lines2_seq[line_num]) > max_len:\n",
    "            continue\n",
    "        lines1_padded = lines1_seq[line_num]+['<pad>']*(max_len-len(lines1_seq[line_num]))\n",
    "        lines2_padded = lines2_seq[line_num]+['<pad>']*(max_len-len(lines2_seq[line_num]))\n",
    "        lines1_seq2idx.append([vocab[i] for i in lines1_padded])\n",
    "        lines2_seq2idx.append([vocab[i] for i in lines2_padded])\n",
    "        \n",
    "        if targets[line_num] == '0':\n",
    "            targets_idx.append([1,0])\n",
    "        else:\n",
    "            targets_idx.append([0,1])\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.LongTensor(lines1_seq2idx).cuda(), torch.LongTensor(lines2_seq2idx).cuda(), torch.LongTensor(targets_idx).cuda()\n",
    "    else:\n",
    "        return torch.LongTensor(lines1_seq2idx), torch.LongTensor(lines2_seq2idx), torch.LongTensor(targets_idx)\n",
    "        \n",
    "\n",
    "\n",
    "lines1_seq2idx, lines2_seq2idx, targets_idx = make_input_seq(train_lines1, train_lines2, train_targets)\n",
    "test_input1, test_input2, test_targets = make_input_seq(test_lines1, test_lines2, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372728\n",
      "372728\n",
      "5297\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(len(lines1_seq2idx))\n",
    "print(len(lines2_seq2idx))\n",
    "print(len(test_targets))\n",
    "print(test_targets.tolist().count([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_input1, test_input2 , test_target):\n",
    "    correct = 0\n",
    "    print(len(test_target))\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in range(len(test_input1)):\n",
    "        test_input1_len = torch.tensor([torch.max(test_input1[i].data.nonzero()+1)])\n",
    "        test_input2_len = torch.tensor([torch.max(test_input2[i].data.nonzero()+1)])\n",
    "        score = model(1, test_input1[i].unsqueeze(0), test_input2[i].unsqueeze(0) , test_input1_len.tolist(), test_input2_len.tolist())\n",
    "        if score.argmax().item() == 1 and test_target[i].argmax().item()==1:\n",
    "            tp+=1\n",
    "        elif score.argmax().item() == 0 and test_target[i].argmax().item()==0:\n",
    "            tn+=1\n",
    "        elif score.argmax().item() == 1 and test_target[i].argmax().item()==0:\n",
    "            fp+=1\n",
    "        elif score.argmax().item() == 0 and test_target[i].argmax().item()==1:\n",
    "            fn+=1\n",
    "        if score.argmax().item() == test_target[i].argmax().item():\n",
    "            correct += 1\n",
    "    try:\n",
    "        precision =  tp/(tp+fp)\n",
    "        recall =  tp/(tp+fn)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "    except:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "    print('precision: {},recall: {},f1 score:{}'.format(precision,recall,f1_score))\n",
    "    print('total: {}, correct: {}'.format(len(test_target), correct))\n",
    "    return correct/len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    compare_regex_model = compare_regex(vocab_size, 4, 256, 2).cuda()\n",
    "else:\n",
    "    compare_regex_model = compare_regex(vocab_size, 4, 256, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297\n",
      "precision: 0,recall: 0,f1 score:0\n",
      "total: 5297, correct: 2500\n",
      "step:0, test acc: 0.4719652633566169\n",
      "epoch: 0, epoch_loss: 0.6589458014956864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-74f6ee54f7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlines1_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines1_seq2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlines2_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines2_seq2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlines1_batch_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines1_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlines2_batch_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines2_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtag_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_regex_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines1_batch_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines2_batch_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-74f6ee54f7c7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlines1_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines1_seq2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlines2_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines2_seq2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlines1_batch_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines1_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlines2_batch_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines2_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtag_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_regex_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines1_batch_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines2_batch_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(compare_regex_model.parameters(), lr=0.1)\n",
    "batch_size = 256\n",
    "\n",
    "batch_num = int(len(lines1_seq2idx)/batch_size)\n",
    "for epoch in range(200):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch in range(batch_num):\n",
    "        compare_regex_model.zero_grad()\n",
    "        lines1_batch = lines1_seq2idx[batch * batch_size:(batch+1) * batch_size]\n",
    "        lines2_batch = lines2_seq2idx[batch * batch_size:(batch+1) * batch_size]\n",
    "        lines1_batch_lengths = torch.tensor([torch.max(lines1_batch[i].data.nonzero()+1) for i in range(len(lines1_batch))]).cuda()\n",
    "        lines2_batch_lengths = torch.tensor([torch.max(lines2_batch[i].data.nonzero()+1) for i in range(len(lines2_batch))]).cuda()\n",
    "        tag_score = compare_regex_model(batch_size, lines1_batch, lines2_batch, lines1_batch_lengths.tolist(), lines2_batch_lengths.tolist())\n",
    "        targets_batches = targets_idx[batch * batch_size:(batch+1) * batch_size]\n",
    "        loss = loss_function(tag_score.squeeze(1).squeeze(1), targets_batches[:,1])\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            test_acc = evaluate_test(compare_regex_model, test_input1, test_input2, test_targets)\n",
    "            print('step:{}, test acc: {}'.format(epoch, test_acc))\n",
    "    \n",
    "    if test_acc == 1.0:\n",
    "        break\n",
    "    print('epoch: {}, epoch_loss: {}'.format(epoch,epoch_loss/batch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "precision: 0.9435665914221218,recall: 0.8444444444444444,f1 score:0.8912579957356077\n",
      "total: 995, correct: 893\n",
      "test acc: 0.8974874371859296\n",
      "epoch: 0, epoch_loss: 6.862730427910782e-07\n"
     ]
    }
   ],
   "source": [
    "batch_num = int(len(lines1_seq2idx)/batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_acc = evaluate_test(compare_regex_model, test_input1, test_input2, test_targets)\n",
    "    print('test acc: {}'.format(test_acc))\n",
    "    \n",
    "    print('epoch: {}, epoch_loss: {}'.format(epoch,epoch_loss/batch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/csrc/generic/serialization.cpp:17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-919ce46fac69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_regex_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./compare_regex_model_share.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_real_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/csrc/generic/serialization.cpp:17"
     ]
    }
   ],
   "source": [
    "torch.save(compare_regex_model, './compare_regex_model_share.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_regex_model = torch.load('./compare_regex_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = ['( ( <M0> ) & ( [ <LET> ] ) ) . * ( [ <CAP> ] ) . *']\n",
    "predict = ['( <M0> ) . * ( ( [ <CAP> ] ) & ( [ <CAP> ] ) ) . *']\n",
    "target = [0]\n",
    "gold_input, predict_input, target_input = make_input_seq(gold, predict, target)\n",
    "gold_len = torch.tensor([torch.max(gold_input[0].data.nonzero()+1)])\n",
    "predict_len = torch.tensor([torch.max(predict_input[0].data.nonzero()+1)])\n",
    "print(gold_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = dict(map(reversed, vocab.items()))\n",
    "print(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([new_vocab[j] for j in [i for i in gold_input.tolist()[0]]]).replace('<pad>','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "with torch.no_grad():\n",
    "    score = compare_regex_model(1, gold_input, predict_input, [gold_len], [predict_len])\n",
    "    print(math.exp(score[0][0]))\n",
    "    print(math.exp(score[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../pair_data/test.txt','r')\n",
    "\n",
    "\n",
    "total_set = set()\n",
    "lines1 = list()\n",
    "lines2 = list()\n",
    "targets = list()\n",
    "\n",
    "count = 0\n",
    "for line in f.read().splitlines():\n",
    "    count += 1\n",
    "    total_set.add(line)\n",
    "    splitted = line.split('\\t')\n",
    "#     total_set.add('{}\\t{}\\t{}'.format(splitted[1],splitted[0],splitted[2]))\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for line in total_set:\n",
    "    count += 1\n",
    "    splitted = line.split('\\t')\n",
    "    lines1.append(splitted[0])\n",
    "    lines2.append(splitted[1])\n",
    "    targets.append(splitted[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input1, test_input2, test_targets = make_input_seq(lines1, lines2, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        print('test acc: {}'.format(evaluate_test(compare_regex_model, test_input1, test_input2,  test_targets)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
